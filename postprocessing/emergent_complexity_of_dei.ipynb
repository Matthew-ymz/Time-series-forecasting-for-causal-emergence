{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_contributions(values):    \n",
    "    \"\"\"  \n",
    "    Returns a list of the same length as 'values'.\n",
    "    - The first output element is the *actual* first value (no subtraction).\n",
    "    - output[0] = current_max = values[0]\n",
    "    - For each subsequent element i:\n",
    "        diff = values[i] - current_max\n",
    "      If diff > 0, output[i] = diff, and update current_max = values[i].\n",
    "      Otherwise, output[i] = 0.\n",
    "    \n",
    "    Example:\n",
    "      If values = [1.2, 1.5, 1.3, 2.0], we get:\n",
    "         output[0] = 1.2\n",
    "         output[1] = 0.3  (since 1.5 - 1.2 = 0.3 > 0)\n",
    "         output[2] = 0    (since 1.3 - 1.5 = -0.2 <= 0)\n",
    "         output[3] = 0.5  (since 2.0 - 1.5 = 0.5 > 0) \n",
    "         and current_max updates in each positive step.\n",
    "    \"\"\" \n",
    "    if not values:\n",
    "        return []\n",
    "    \n",
    "    output = []\n",
    "    # First element is just the first input value\n",
    "    output.append(values[0])\n",
    "    \n",
    "    # Initialize current_max to the first input\n",
    "    current_max = values[0]\n",
    "    \n",
    "    # For each subsequent value, compute diff\n",
    "    for val in values[1:]:\n",
    "        diff = val - current_max\n",
    "        if diff > 0:\n",
    "            output.append(diff)\n",
    "            current_max = val\n",
    "        else:\n",
    "            output.append(0)\n",
    "    return output\n",
    "\n",
    "def compute_entropy(increments):\n",
    "    \"\"\"\n",
    "    Given a list of positive increments, normalizes them into a probability\n",
    "    distribution and computes the Shannon entropy in bits.\n",
    "\n",
    "    Entropy = -Î£ p_i * log2(p_i)     \n",
    "    where p_i = increments[i] / sum(increments).    \n",
    "    If increments is empty or sums to zero, returns 0.0.   \n",
    "    \"\"\"    \n",
    "    if not increments:\n",
    "        return 0.0\n",
    "    \n",
    "    total = sum(increments)\n",
    "    # If total is 0, there's no variation => 0.0 entropy\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Normalize to probabilities\n",
    "    probabilities = [x / total for x in increments]\n",
    "\n",
    "    # Compute Shannon entropy (base 2)\n",
    "    entropy = 0.0\n",
    "    for p in probabilities:\n",
    "        # Only compute for p > 0 to avoid math domain errors\n",
    "        if p > 0:\n",
    "            entropy -= p * math.log2(p)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "dei_list = [1, 0.9, 1.6, 4]\n",
    "contri_list = get_positive_contributions(dei_list)\n",
    "entropy = compute_entropy(contri_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
