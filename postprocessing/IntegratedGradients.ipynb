{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = 7\n",
    "folder_name = 'sev'\n",
    "device = torch.device('cuda:%s'%cuda)\n",
    "stage_scale_list = []\n",
    "def cal_integrate_gradients():\n",
    "    print(mice_id)\n",
    "    attribution_array1, attribution_array2  = [], []\n",
    "    train_input_list, train_target_list, test_input_list, test_target_list = [], [], [], []\n",
    "    test_input = pd.read_csv('./loc_data_allstage/%s/%s/stage%s/test_input.csv'%(folder_name, mice_id, stage), header=None)\n",
    "    test_input_list.append(np.array(test_input))\n",
    "    test_target = pd.read_csv('./loc_data_allstage/%s/%s/stage%s/test_target.csv'%(folder_name, mice_id, stage), header=None)\n",
    "    test_target_list.append(np.array(test_target))\n",
    "    test_input_list = np.concatenate(np.array(test_input_list),axis=0)\n",
    "    test_target_list = np.concatenate(np.array(test_target_list),axis=0)\n",
    "    input_data = torch.tensor(test_input_list, dtype=torch.float32, device=device) # [1,1000,64]\n",
    "    target_data = torch.tensor(test_target_list, dtype=torch.float32, device=device) # [1,1000,64]\n",
    "    input = input_data[:800]\n",
    "    target = target_data[:800]\n",
    "    hidden_units = 100\n",
    "    scale = 1\n",
    "    batch_size = 512\n",
    "    MAE = torch.nn.L1Loss(reduction='none')\n",
    "    MSE = torch.nn.MSELoss(reduction='none')\n",
    "    #Define the net\n",
    "    scale_std = []\n",
    "    scale_list = [32,16,8,4,2,1]\n",
    "    value_index = range(32)\n",
    "    net = Parellel_Renorm_Dynamic(sym_size = input_data.shape[1], latent_size = scale, effect_size = input_data.shape[1],cut_size=2, hidden_units = 100, normalized_state = True, device=device)\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(f'./loc_model_stage2/{folder_name}/{mice_id}/stage{stage}_scale%s.pkl'%weight_id))        \n",
    "    feature_number = scale_list[weight_id]\n",
    "    \n",
    "\n",
    "    def encoding_function(input_data):\n",
    "        return net.encoding2(input_data, weight_id)[-1]\n",
    "        \n",
    "        ig = IntegratedGradients(encoding_function)\n",
    "        group_number = pd.read_csv('./loc_data_allstage/%s/%s/%s/group_number.csv'%(folder_name, mice_id, 'stage1'), header=None)\n",
    "        brain_id = pd.read_csv('./loc_data_allstage/%s/%s/%s/brain_id.csv'%(folder_name, mice_id, 'stage1'), header=None)\n",
    "        loc_data = pd.read_csv('./loc_data_allstage/%s/%s/%s/mean_loc.csv'%(folder_name, mice_id, 'stage1'), header=None)\n",
    "        cluster_idx = pd.read_csv('./loc_data_allstage/%s/%s/%s/cluster_idx.csv'%(folder_name, mice_id, 'stage1'), header=None)\n",
    "        loc_axis = pd.read_csv('./loc_data_allstage/%s/%s/%s/loc_axis.csv'%(folder_name, mice_id, 'stage1'), header=None)\n",
    "        \n",
    "        loc_data = np.array(loc_data)\n",
    "        cluster_idx = np.array(cluster_idx)\n",
    "        loc_axis = np.array(loc_axis)\n",
    "        group_number = np.array(group_number)[:,0]\n",
    "        brain_id = np.array(brain_id)[:,0]\n",
    "        group_cum = np.cumsum(group_number)\n",
    "        group_cum = np.concatenate((np.array([0]), group_cum))\n",
    "        attri_neuro = np.zeros(cluster_idx.shape[0])\n",
    "        \n",
    "        brain_area_number = list(dict(count_and_sort_elements(brain_id)).values())\n",
    "        temp_std = []\n",
    "        for target in range(feature_number):\n",
    "            attributions, approximation_error = ig.attribute(input,target=target,method='gausslegendre',return_convergence_delta=True)\n",
    "            attributions = (attributions.abs().cpu().numpy()).mean(0)\n",
    "        attribution_array1.append(attributions)\n",
    "        for temp_id in range(len(group_cum)-1):\n",
    "            # attri_neuro[group_cum[temp_id]:group_cum[temp_id+1]] = attributions[temp_id] / group_number[temp_id]\n",
    "            attri_neuro[group_cum[temp_id]:group_cum[temp_id+1]] = attributions[temp_id]\n",
    "        attribution_array2.append(attri_neuro)\n",
    "attribution_array1_all.append(attribution_array1)\n",
    "attribution_array2_all.append(attribution_array2)\n",
    "    \n",
    "# attribution_array1_all = np.array(attribution_array1_all)\n",
    "# attribution_array2_all = np.array(attribution_array2_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ymz_torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
